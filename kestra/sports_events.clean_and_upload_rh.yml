id: clean_and_upload_rh
namespace: sports_events

inputs:
  - id: rh_raw_file
    type: STRING

tasks:

  # 1. CLEAN RH (Python)
 
  - id: clean_rh_python
    type: io.kestra.plugin.scripts.python.Script
    taskRunner:
      type: io.kestra.plugin.scripts.runner.docker.Docker
      image: ghcr.io/kestra-io/pydata:latest
    beforeCommands:
      - pip install --no-cache-dir unidecode

    script: |
      import pandas as pd
      import json
      import unidecode

      # Load raw file
      raw_df = pd.read_csv("{{ inputs.rh_raw_file }}")
      
      # Safe parse
      def safe_parse(x):
          if isinstance(x, dict):
              return x
          if isinstance(x, str):
              return json.loads(x)
          raise ValueError(f"Unexpected type in _airbyte_data: {type(x)}")
      
      raw_df["_airbyte_data"] = raw_df["_airbyte_data"].apply(safe_parse)
      df = pd.json_normalize(raw_df["_airbyte_data"])

      # Normalize values

      def normalize_value(text):
          if pd.isna(text):
              return None
          text = unidecode.unidecode(text)
          text = text.lower()
          text = text.replace("/", "_")
          text = text.replace("-", "_")
          text = text.replace(" ", "_")
          return text

      
      # Normalize columns
      df.columns = (
          df.columns
            .str.strip()
            .str.lower()
            .str.normalize('NFKD')
            .str.encode('ascii', errors='ignore')
            .str.decode('utf-8')
            .str.replace(' ', '_')
            .str.replace("/","_")
      )

      # Order columns
      cols_ordered = [
        'employee_id','nom','prenom','jours_cp','type_contrat',
        'departement','salarie_brut','date_naissance',
        'moyen_deplacement','addresse_domicile','date_embauche'
      ]

      # Cleaning
      df["moyen_deplacement"] = df["moyen_deplacement"].apply(normalize_value)
      df = df.drop_duplicates(subset=["employee_id"])
      df["date_naissance"] = pd.to_datetime(df["date_naissance"], errors="coerce")
      df["date_embauche"] = pd.to_datetime(df["date_embauche"], errors="coerce")
      df["salarie_brut"] = pd.to_numeric(df["salarie_brut"], errors="coerce")
      df = df[df["employee_id"].notnull()]

      df = df[[c for c in cols_ordered if c in df.columns]]

      # Save output
      df.to_csv("donnee_RH_clean.csv", index=False)

    inputFiles: {}
    outputFiles:
      - "donnee_RH_clean.csv"

 
  # 2. UPLOAD CLEAN FILE TO S3

  - id: upload_clean_rh
    type: io.kestra.plugin.aws.s3.Upload
    from: "{{ outputs.clean_rh_python.outputFiles['donnee_RH_clean.csv'] }}"
    bucket: sport-data-raw
    key: "clean/hr/donnee_RH_clean.csv"
    accessKeyId: "AKIAUK6NAZYZSJY7Y4IG"
    secretKeyId: "AyZ2XtiJAR0HiiNSmAptPOyX0THVGHNpi5sgiZUS"
    region: "eu-west-3"


# TRIGGER â€” Start after Flow 1 completes
outputs:
  - id: rh_clean_file
    type: STRING
    value: "{{ outputs.clean_rh_python.outputFiles['donnee_RH_clean.csv'] }}"

triggers:
  - id: trigger_after_extract_rh
    type: io.kestra.plugin.core.trigger.Flow
    preconditions:
      id: rh_extract_done
      flows:
        - namespace: sports_events
          flowId: extract_rh_raw_from_s3
          states: [SUCCESS]
    inputs:
      rh_raw_file: "{{ outputs.download_rh_raw.uri }}"


