# Kestra â€” Orchestration du Pipeline (POC Sport Data Solution)

Ce projet contient les **14 workflows Kestra** utilisÃ©s pour orchestrer lâ€™ensemble du pipeline de traitement de donnÃ©es dans le cadre du **POC Sport Data Solution**.

Lâ€™orchestration est exÃ©cutÃ©e **localement via Docker Compose**.  
âš ï¸ Les credentials sont **enlevÃ© volontairement** dans les fichiers de flow pour s'assurer la sÃ©curitÃ©

## ğŸ“š Table des matiÃ¨res

- [Structure du projet](#structure-du-projet)
- [Lancement de Kestra](#lancement-de-kestra)
- [Gestion des credentials (POC)](#gestion-des-credentials-poc)
- [Description des workflows](#description-des-workflows)
  - [Extraction](#extraction)
  - [Nettoyage](#nettoyage)
  - [Validation](#validation)
  - [Zone Curated](#zone-curated)
  - [Chargement](#chargement)
  - [Notification](#notification)
- [Ordre d'exÃ©cution du pipeline](#ordre-dexÃ©cution-du-pipeline)
- [ExÃ©cution manuelle d'un workflow](#exÃ©cution-manuelle-dun-workflow)
- [RÃ©sumÃ©](#rÃ©sumÃ©)

---

## ğŸ“ Structure du projet

```
kestra/
â”‚â”€â”€ docker-compose.yml
â”‚â”€â”€ README.md
â””â”€â”€ flows/
    â”œâ”€â”€ clean_and_upload_activities.yml
    â”œâ”€â”€ clean_and_upload_rh.yml
    â”œâ”€â”€ clean_and_upload_sport.yml
    â”œâ”€â”€ curated_dim_employees.yml
    â”œâ”€â”€ curated_fact_activities.yml
    â”œâ”€â”€ curated_fact_avantages.yml
    â”œâ”€â”€ extract_activities_raw_from_s3.yml
    â”œâ”€â”€ extract_rh_raw_from_s3.yml
    â”œâ”€â”€ extract_sport_raw_from_s3.yml
    â”œâ”€â”€ load_curated_to_rds.yml
    â”œâ”€â”€ run_all_extracts.yml
    â”œâ”€â”€ send_slack_activity.yml
    â”œâ”€â”€ validate_activities_clean.yml
    â””â”€â”€ validate_rh_clean.yml
```

---

## ğŸ”§ Lancement de Kestra

Depuis le dossier `kestra/`, lancez l'orchestrateur localement :

```bash
docker compose up -d
```

AccÃ©dez Ã  lâ€™interface Kestra ici :  
ğŸ‘‰ [http://localhost:8080](http://localhost:8080)

Pour arrÃªter lâ€™environnement :

```bash
docker compose down
```

---

## ğŸ” Gestion des credentials (POC)

Les identifiants sont **enlevÃ©s dans les fichiers de workflow** pour des raisons de sÃ©curitÃ© :

- Credentials AWS (S3)
- Identifiants PostgreSQL (RDS)
- Webhook Slack
- ClÃ© API Google Maps (si utilisÃ©e)

---

## ğŸ”„ Description des Workflows

### ğŸŸ¦ Extraction

| Workflow | Description |
|----------|-------------|
| `extract_activities_raw_from_s3` | Extraction des donnÃ©es brutes dâ€™activitÃ©s sportives depuis S3 |
| `extract_rh_raw_from_s3` | Extraction des donnÃ©es brutes RH depuis S3 |
| `extract_sport_raw_from_s3` | Extraction dâ€™un second dataset sport depuis S3 |
| `run_all_extracts` | ExÃ©cute automatiquement les trois extractions ci-dessus |

### ğŸŸ© Nettoyage

| Workflow | Description |
|----------|-------------|
| `clean_and_upload_activities` | Nettoyage du dataset "activities" et upload en version nettoyÃ©e |
| `clean_and_upload_rh` | Normalisation complÃ¨te des donnÃ©es RH (format, noms, dates) |
| `clean_and_upload_sport` | Nettoyage du dataset brut "sport" |

### ğŸŸ¨ Validation

| Workflow | Description |
|----------|-------------|
| `validate_activities_clean` | ContrÃ´les qualitÃ© post-nettoyage : types, nulls, cohÃ©rences |
| `validate_rh_clean` | ContrÃ´les qualitÃ© sur les donnÃ©es RH nettoyÃ©es |

### ğŸŸ§ Zone Curated

| Workflow | Description |
|----------|-------------|
| `curated_dim_employees` | Construction de la dimension "employÃ©s" Ã  partir des RH enrichies |
| `curated_fact_activities` | CrÃ©ation de la table de faits des activitÃ©s nettoyÃ©es et enrichies |
| `curated_fact_avantages` | Calcul des avantages :<br>â€¢ prime 5% si trajet sportif<br>â€¢ 5 jours bien-Ãªtre si activitÃ© soutenue |

### ğŸŸ« Chargement

| Workflow | Description |
|----------|-------------|
| `load_curated_to_rds` | Chargement des tables curated (DIM et FACT) dans PostgreSQL RDS |

### ğŸŸª Notification

| Workflow | Description |
|----------|-------------|
| `send_slack_activity` | DÃ©tection de la prochaine activitÃ© non envoyÃ©e + notification Slack + log dans PostgreSQL |

---

## ğŸ” Ordre d'exÃ©cution du pipeline

Le pipeline suit une exÃ©cution **en cascade**, automatisÃ©e par les **triggers internes de Kestra** :

```
Extraction
   â†“
Nettoyage
   â†“
Validation
   â†“
Zone Curated
   â†“
Chargement (RDS)
   â†“
Notification Slack
```

---

## ğŸ§ª ExÃ©cution manuelle dâ€™un workflow

Dans lâ€™interface graphique Kestra :

1. AccÃ©dez au menu **Flows**
2. SÃ©lectionnez le workflow Ã  exÃ©cuter
3. Cliquez sur **RUN**

> Tous les workflows sont indÃ©pendants et peuvent Ãªtre lancÃ©s manuellement ou automatiquement via orchestration.

---

## ğŸ“˜ RÃ©sumÃ©

Ce projet contient :

- âœ… **14 workflows Kestra** prÃªts Ã  lâ€™emploi
- âœ… Une **orchestration complÃ¨te** du pipeline de donnÃ©es
- âœ… Lâ€™intÃ©gration complÃ¨te **S3 â†’ Kestra â†’ PostgreSQL (RDS) â†’ Slack**
- âœ… Une stack exÃ©cutable en local via **Docker Compose**
- âœ… Des identifiants hardcodÃ©s (temporairement) pour usage en **POC privÃ©**
